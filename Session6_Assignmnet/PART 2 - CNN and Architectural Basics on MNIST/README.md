# MNIST Digit Classification 

In this assignment, we will be using the MNIST data for classifying handwritten digits using a convolutional layer. 

## Requirements

1. 99.4% validation accuracy
2. Less than 20k Parameters
3. Less than 20 Epochs
4. Use Batch Normalization and Dropout
5. A Fully connected layer and have used GAP (Optional)

## Introduction

#### Transition Layer
Transition layers (convolution + pooling), is a way of downsampling the representations calculated by convolution blocks slowly upto the end as after transition layers the representations go from 28×28 to 14×14
1. Max pooling - Max pooling is a pooling operation used in convolutional neural networks (CNNs) for feature extraction.It filters out least importenet features and sends out most importent features to consecutive layers for prediction.
2. Convolution of 1x1 - 1x1 convolution is often used as a transition block in convolutional neural networks (CNNs) to adjust the number of channels (also known as the depth or feature maps) between two consecutive layers. It is commonly used to reduce the number of channels before applying a larger convolution or pooling operation, or to increase the number of channels after a pooling operation and also introduces new parameters and new non-linearity into the network that will help to increase model accuracy. 

#### Batch Normalization
Batch Normalization (BN) is a technique commonly used in deep neural networks to improve the training and performance of models. It normalizes the activations of each layer across a mini-batch of training examples, hence the name "batch" normalization.

#### Global Average Pooling
Global Average Pooling (GAP) is a pooling operation commonly used in convolutional neural networks (CNNs) for image classification tasks. It is a way to reduce the spatial dimensions of the feature maps generated by the convolutional layers, resulting in a compact and fixed-length representation of the input image
#### Dropout
Dropout refers to a regularization technique used during the training phase to prevent overfitting. Overfitting occurs when a neural network becomes too specialized in learning the training data and performs poorly on unseen or test data.

## Model Architecture
The model consists of different layers of convolutional layers, max pooling, batch normalization, dropout, global average pooling. 

### MNIST Data
The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.The database is also widely used for training and testing in the field of machine learning. It is made up of a number of grayscale pictures that represent the digits 0 through 9. The collection contains square images that are each 28x28 pixels in size, for a total of 784 pixels per image.

MNIST features are best extracted at edges,gradients 

![Screenshot-from-2021-03-12-15-21-04](https://github.com/prarthanats/ERA/assets/32382676/e12f00f2-801d-47d6-a4f4-c41e07a20d17)

Pixels on the edge have a significant difference in values. We can compare neighboring pixel values to find the edge. we can clearly identify the edges by looking at the numbers or the pixel values. So if you look closely in the matrix of the numbers, there is a significant difference between the pixel values around the edge. The black area in the left image is represented by low values as shown in the second image. Similarly, the white area is represented by the larger numbers.

## Model Architecture
Tried to implement a  Squeeze-and-Excitation network to identify the numbers in MINIST dataset. This architecute consist of convolution blocks followed by transition blocks. This is a technique designed to enhance the representational power of convolutional neural networks (CNNs) by explicitly modeling interdependencies between channels. The depth of a feature map represents different channels or filters, which capture various patterns and features in the input data. The SE network aims to adaptively recalibrate the feature maps by assigning different importance weights to different channels.

![download (1)](https://github.com/prarthanats/ERA/assets/32382676/0c48d16e-6de4-42f2-ba3e-c7a6bd9b66c8)

1. Convolution Block: Convolution layer consists of channels of size 8, 16, 32.The implemented architecture is designed to extract edges and gradients at the Receptive Fields of 3 to 7,with 3 convolution blocks and convolution block 4 for textures. 
2. Transition Block: To reduce the channel after each block, from 32 to 8. after convolution block, 1x1 convolution is applied , that helped us to reduce num of parameters. This is squeeze operation.
3. AvgPooling: It is applied as the 2nd pool before prediction so as to calculates for each pixel is average value rather than the max value for the feature map
4. BatchNormalization: It is applied after every convolution layer except the last one to standadize the input to a convolution layer for every batch. 
5. Dropout: It is applied in the 1st convolution and convolution 4 to reduce the overfitting

## Model Summary

The total parameters of the model is around 15K.

<img width="341" alt="Model_Summary" src="https://github.com/prarthanats/ERA/assets/32382676/38e596f2-c209-4769-8243-bde70f3382be">

Model consists of convolution and transition blocks batch size of 32, epoch as 20 and optimizer as SGD.

1. First convolution Block consists of 3 convolution layers of channel size 8, 16 & 32, with batch normalization, zero padding and dropout
2. First Transition Block consist of Max Pooling layer of 2x2 followed by 1x1 convolution with input as 32 and output as 8 channels. 
3. Second Convolution Block consist of 2 convolution layers of channel size 16 & 32, with batch normalization, zero padding and dropout
4. One layer of pooling using AveragePooling.
5. Final convolution layer with channel size from 20 to 10 with 3x3 kernel


## Output
Reached highest testing accuracy as 99.4% at 12th epoch.

<img width="644" alt="output" src="https://github.com/prarthanats/ERA/assets/32382676/cd266714-d285-48cc-a964-98122bb201fa">
